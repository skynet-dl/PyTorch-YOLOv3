****
# День 4. Распознавание образов и компьютерное зрение <a name="5"></a>

Большая часть информации медицинского характера накапливается в графическом виде: снимки УЗИ, рентгеновские снимки, снимки эндоскопов и пр. Поэтому не менее важно использовать средств ИИ для распознавания образов на графической информации.

В этом задании мы рассмотрим средства и алгоритмы компьютерного зрения и решим тестовую задачу распозавания объектов на серии фотографий с использованием библиотек `Tensorflow` и `Keras`. 

## Нейронная сеть <a name="5_1"></a>

Нейронной сетью называется математическая модель, реализующая фукнции искусственного интеллекта путём воспроизведения нервной системы человека. Они используются для решения сложных задач, которые требуют аналитических вычислений, подобных тем, что делает человеческий мозг. К таким задачам относятся, например, классификация, кластеризация, прогнозирование, распознавание и т.д.

Искусственный нейрон представляет собой сумматор входных сигналов, применяющий к полученной взвешенной сумме некоторую простую функцию. Нейрон имеет синапсы - однонаправленные входные связи, соединённые с выходами других нейронов, а также аксон - выходную связь.

<img src="assets/artificial-neuron.PNG" width="600">
**Схема искусственного нейрона**
 

Текущее состояние нейрона определяется взвешенной суммой его входов (см. схему). Выход нейрона определяется его активационной функцией.
Существует несколько вариантов функций активанции.

<img src="assets/image040.jpg" width="500">


Чаще всего для свёрточных нейронных сетей используются сигмоидальные функции или их приближения:

-	логистическая функция;

-	гиперболический тангенс;

-	линейно-пороговая функция.

Сигмоидальные и подобные им функции хороши тем, что позволяют усиливать слабые сигналы и не насыщаться от сильных сигналов. 


Совокупность нейронов, расположенных на одном уровне в нейронной сети, называется слоем. В общем случае нейронная сеть включает в себя входной, выходной и промежуточные слои. Нейроны входного и выходного слоёв, как правило, имеют линейную функцию активации и предназначены для приёма и передачи данных. Нейроны промежуточных слоёв - нелинейные; их функцией активации чаще всего является сигмоид (логистическая функция):
![equation](https://latex.codecogs.com/png.latex?f%28x%29%20%3D%20%5Cfrac%7B1%7D%7B1%20&plus;%20e%5E%7B-%5Calpha%20x%7D%7D)

На схеме показан пример полносвязной нейронной сети, имеющей входной, промежуточный и выходной слои.

<img src="assets/image073.png" width="600">



Нейронная сеть обучаема. В процессе обучения параметры сети настраиваются в соответствии с обучающими наборами данных, моделирующих среду, в которой будет функционировать сеть. В зависимости от способа подстройки параметров различают обучение с учителем и без учителя.

Обучение с учителем представляет собой предъявление сети выборки обучающих примеров. Каждый образец подаётся на входы сети, проходит обработку и перерабатывается в выходной сигнал, который сравнивается с эталонным значением. Затем в зависимости от степени расхождения реального и идеального результатов изменяются весовые коэффициенты связей внутри сети. Обучение длится до тех пор, пока ошибка по всему обучающему массиву не достигнет приемлемо низкого уровня.

При обучении без учителя обучающее множество состоит лишь из входных векторов. Алгоритм обучения подстраивает веса внутри сети так, чтобы предъявление достаточно близких входных векторов давало одинаковые результаты.

Почитать подробнее про нейронные сети можно [здесь](http://www.aiportal.ru/articles/neural-networks "Статьи о нейронных сетях").



## Сверточные нейронные сети <a name="5_2"></a>

*Свёрточная нейронная сеть* — специальная архитектура искусственных нейронных сетей, основанные на представлении изображений в виде тензоров и нацеленная на эффективное распознавание образов в графических изображениях. Тензоры — это 3-х мерные массивы.

Для распознавания образов могут использоваться и простые модели нейронных сетей, такие как  многослойный персептрон. Однако, если размеры изображения велики, то число и сложность слоев нейронной сети многократно увеличивается, а процесс обучения существенно усложняется. Другим недостатком многослойных нейронных сетей является векторный характер представления данных, что делает невозможный двумерную локализации пикселей и обработку деталей на изображении.

Как и полносвязная нейронная сеть, свёрточная сеть обучается с помощью алгоритма обратного распространения ошибки. Сначала выполняется прямое распространение от первого слоя к последнему, после чего вычисляется ошибка на выходном слое и распространяется обратно. При этом на каждом слое вычисляются градиенты обучаемых параметров, которые в конце обратного распространения используются для обновления весов с помощью градиентного спуска.

Свёрточные нейронные сети состоят из последовательно соединенных слоев нескольких типов, выполняющими преобразования над поступающей матрицей или несколькими матрицами (например, исходное изображение представляется тремя матрицами R,G и B компонент цветности). Обычно, свёрточные НС построены на чередовании свёрточных слоёв, реализующих функцию свёртки, и субдискретизирующих слоёв, ответственных за выборку наиболее подходящего раздражителя — и тем самым уменьшающих размер обрабатываемого изображения. Также в сверточных НС используются элементы полносвязных персептронов: слои активации и полносвязные слои. Из слоев различных типов можно конструировать НС, наиболее подходящие для каждой конкретной задачи. 

### Серточный слой <a name="5_2_1"></a>

Когда входное изображение поступает на этот слой, к нему применяется операция свёртки, которая заключается в перемножении элементов фрагмента изображения с соответствующими элементами ядра свёртки и записью результата напротив центрального элемента фрагмента. Следующий пример показывает, как в практических приложениях производится операция свёртки над изображением с ядром размером 3х3:
 
<img src="assets/image050.gif" width="800">

где буквы *a - i* — соответствующие пиксели фрагмента изображения, цифры 1-9 — соответствующие коэффициенты ядра функции свёртки, [2,2] — координаты элемента, на место которого необходимо вставить получившуюся сумму попарных произведений.

В этом слое операция свёртки производится параллельно над каждым пикселем изображения. Если пиксель находится в углу или так, что одному или нескольким коэффициентам ядра нет соответствующего пикселя, применяют одну из двух стратегий:

а.	недостающие пиксели заполняются тем же значением, что и ближайший к нему пиксель изображения;

б.	такие пиксели отбрасываются и выходное изображение получается несколько меньшего размера.

Результаты применения стратегии a) свертки и ядра 2x2
![equation](https://latex.codecogs.com/gif.latex?%5Cdpi%7B200%7D%20%5Ctiny%20%5Cbegin%7Bpmatrix%7D%201%20%26%202%5C%5C%203%20%26%204%20%5Cend%7Bpmatrix%7D)
показаны соответственно на рисунках:

<img src="assets/image062.png" width="500">

<img src="assets/image063.png" width="250">
**Результаты применений стратегий а) свертки к изображению.**


Иногда после свёрточного слоя вставляется так называемый слой *ReLU — Rectified Linear Unit* — блок линейной свёртки. Он состоит из матрицы активационных функций размером с выходное изображение. Активационные функции выбираются чаще всего не сигмоидальные, а ненасыщаемые функции вида 

![equation](https://latex.codecogs.com/gif.latex?F_%7Bact%7D%28y%29%3Dmax%20%5Cleft%20%28%200%2Cy%20%5Cright%20%29)

или

![equation](https://latex.codecogs.com/gif.latex?F_%7Bact%7D%28y%29%3D%20%5Cbeta%20e%5E%7B-%5Calpha%20y%7D). 

Цель применения свёрточного слоя — выявить общие детали множества изображений. Так как ядра свёртки обучаются под общие детали, влияние шума на них минимизируется по сравнению с многослойным перцептроном. В них для каждого входного слоя есть соединение с каждым нейтроном на входном слое, тем самым способствуя обучению и общим и частным деталям.

Таким образом использование свёрточных слоёв позволяет расположить нейросеть к выделению более общих частей по сравнению с перцептроном, вместе с тем занимая меньше места при хранении и в оперативной памяти, так как хранятся не сонмы связей, а небольшое количество ядер свёртки.

Добавление блока ReLU позволяет ещё сильнее снизить влияние входно-го шума на выход слоя, тем самым упрощая работу при обучении.



### Субдискретизирующий слой <a name="5_2_2"></a>

Слой субдискретизации служит для уменьшения изображения посредством нелинейных преобразований. Идея, положенная в основу создания этого слоя такова: когда свёрточный слой выявил признаки, чрезмерно высокую детализацию можно отсечь и получить на выходе только общую картину.

Этот слой не является необходимым в свёрточных нейронных сетях, и иногда опускается, однако его использование даёт выигрыш по скорости работы и обучения слоёв, идущим за ним. Можно сказать, что этот слой помогает свёрточной нейронной сети абстрагироваться от деталей и частично увидеть общую картину.

Слой субдискретизации характеризуется следующими параметрами:

а)	размер субдискретизируемой группы пикселей – обычно 2х2;

б)	шаг применения функции дискретизации – обычно равен ширине субдискретизируемой группы пикселей, то есть двум;

в)	функция субдискретизации – обычно выбирается максимальное значение из группы пикселей, но бывает выборка минимума или выборка среднего арифметического значения.

Принцип работы этого слоя представлен на рисунке ниже: к группе пиксе-лей, например, 2х2, применяется функция субдискретизации, в текущем случае – выборка максимального значения. Результат функции субдискретизации, применённой к группе пикселей входного изображения, записывается в единственный пиксель выходного изображения.

<img src="assets/image070.png" width="500">
**Результат применения слоя субдискретизации к изображению**

Если перед субдискретизирующим слоем уже стоял блок ReLU, бессмысленно ставить такой же после этого слоя, так как выходное значение не изменится. Если блок ReLU будет использовать иную активационную функцию, значение изменится, но того же эффекта можно было бы добиться, изменив функцию предыдущего блока.

Таким образом, после слоя субдискретизации обыкновенно не ставят слой ReLU. Применение субдискретизирующего слоя заключается в упрощении дальнейшей обработки изображения, проходящего по конвейеру и в вычленении общих деталей изображения.


### Обучение НС <a name="5_2_3"></a>

Процесс некоторого определённого изменения весов связей нейронов и значений элементов свёрточных слоёв с целью получения некоторой опреде-лённой реакции нейронной сети на входные данные именуется обучением НС. Оно происходит до тех пор, пока не наступит определённое условие, чаще всего – процент ошибок на обучающем наборе должен опуститься ниже определённого значения.

Ниже приведена классификация способов обучения НС:

а.	по наличию элемента случайности:

  1.	детерминистские методы – процедура обучения НС основана на использовании текущих значений весов нейронов и элементов ядер свёртки и желаемом выходе сети;

  2.	стохастические методы – процедура обучения НС основана на случайном изменении весов в соответствии с определён-ной функцией распределения;

б.	по способу определения корректности результата:

  1.	с учителем – процедура обучения НС основана на одновре-менной подаче входных и выходных данных. Если выход нейронной сети совпадает с требуемым, то берётся следую-щий набор данных, в противном случае нейронная сеть до-обучается;

  2.	с последовательным подкреплением знаний – процедура обучения НС основана на подаче входных данных и последующей оценке выходных данных. Оценка бывает либо вида «хорошо-плохо», либо численной;

  3.	без учителя – .процедура обучения НС основана на подаче входных данных, а дальше нейронная сеть сама пытается вы-членить особенности входных данных.

Методов обучения, попадающих под эту классификацию, очень много, однако для используемой свёрточной НС наиболее подходящий и наиболее популярный – метод обратного распространения ошибки. Он применим только к НС с прямым распространением сигнала, то есть без обратных связей, и используемая НС является таковой.

Алгоритм обратного распространения ошибки следующий:

а.	инициализировать веса нейронов и фильтров маленькими случай-ными значениями;

б.	выбрать очередную обучающую пару из обучающего множества; подать входной вектор на вход сети;

в.	вычислить выход сети;

г.	вычислить разность между выходом сети и требуемым выходом;

д.	подкорректировать веса сети для минимизации ошибки;

е.	Повторять шаги со второго по пятый для каждой пары обучающего множества до тех пор, пока ошибка на всем множестве не достигнет приемлемого уровня.

Шаги б) и в) образуют так называемый «проход вперёд», так как сигнал распространяется по сети от входа к выходу. Шаги г) и д) составляют «обратный проход», здесь вычисляемый сигнал ошибки распространяется обратно по сети и используется для подстройки весов.

При обучении НС следует избегать переобучения НС, когда на обучаю-щей выборке ошибок почти нет, а на реальных данных возникает крайне силь-ное расхождение, то есть стоит избегать слишком сильной «подгонки» резуль-татов работы НС под входные данные.


## Библиотека Tensorflow <a name="5_3"></a>

*TensorFlow*  — это фреймворк машинного обучения, предоставляющий разработчикам функционал для сбора данных, а также для построения и обучения моделей, основанных на нейронных сетях. 
TensorFlow позволяет разработчикам создавать специальные обрабатывающие структуры - графы потоков данных, которые описывают, как данные перемещаются через последовательность узлов обработки. Каждый узел в графе представляет математическую операцию, а каждое соединение между узлами представляет собой многомерный массив данных или тензор.

TensorFlow использует синтаксис языка Python, т.к. он прост в освоении и предоставляет удобные способы выражения алгоритмов действий в виде высокоуровневых абстракций. Узлы и тензоры в TensorFlow являются объектами Python, а приложения TensorFlow являются Python-приложениями. Однако, для повышения производительности обработки данных, TensorFlow написан на языке C++ и скомпилирован в бинарные файлы. Python используется лишь для упрощения синтаксических конструкций и использования высокоуровневых программных абстракций.

Приложения TensorFlow можно запускать как на локальном компьютере, так и в облачном кластере, на устройствах iOS и Android, на мобильных процессорах, встраиваемых системах типа RaspberryPi или графических процессорах. Полученные в результате исследований модели TensorFlow могут быть развернуты на любом устройстве, где они будут использоваться для рапознавания и формирования прогнозов.



## Пример использования Tensorflow и Keras<a name="5_4"></a>

Мы разработаем программный код для распознавания объектов на фотографиях. Будем использовать небольшую обучающую выборку задачи распознавания :

<img src="assets/cv_01.jpg" width="500">
**Классика жанра**

Загрузите архив с графическими файлами на сервер:

```shell
wget https://github.com/alexbmstu/2019/raw/master/data/chihuahua-muffin.zip
unzip chihuahua-muffin.zip
cd chihuahua-muffin
```

Вы можете использовать терминал из Jupyter Notebooks.

![](assets/cv_02.png)
**Терминал Jupyter Notebooks**

или перенесите файлы любым другим способом:

- [Изображения](data/chihuahua-muffin.zip)

Установим бибилиотеку Tensorflow и Keras:

```shell
conda upgrade conda
conda upgrade --all
pip3 install --upgrade tensorflow matplotlib pillow
```

Подключим необходимые библиотеки Tensorflow и keras

```python
 # TensorFlow and tf.keras
import tensorflow as tf
from tensorflow import keras
 #import keras.utils
from keras import utils as np_utils
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.optimizers import SGD

 # Helper libraries
import numpy as np
import matplotlib.pyplot as plt
import glob, os
import re

 # Pillow
import PIL
from PIL import Image
```

Цветность изоражения необходимо преобразовать в 8-бит Grayscale. Также приведем все изображения к одинаковому размеру 100х100 пикселей.

```python
 # Use Pillow library to convert an input jpeg to a 8 bit grey scale image array for processing.
def jpeg_to_8_bit_greyscale(path, maxsize):
        img = Image.open(path).convert('L')   # convert image to 8-bit grayscale
        # Make aspect ratio as 1:1, by applying image crop.
    # Please note, croping works for this data set, but in general one
    # needs to locate the subject and then crop or scale accordingly.
        WIDTH, HEIGHT = img.size
        if WIDTH != HEIGHT:
                m_min_d = min(WIDTH, HEIGHT)
                img = img.crop((0, 0, m_min_d, m_min_d))
        # Scale the image to the requested maxsize by Anti-alias sampling.
        img.thumbnail(maxsize, PIL.Image.ANTIALIAS)
        return np.asarray(img)
    
def load_image_dataset(path_dir, maxsize):
        images = []
        labels = []
        os.chdir(path_dir)
        for file in glob.glob("*.jpg"):
                img = jpeg_to_8_bit_greyscale(file, maxsize)
                if re.match('chihuahua.*', file):
                        images.append(img)
                        labels.append(0)
                elif re.match('muffin.*', file):
                        images.append(img)
                        labels.append(1)
        return (np.asarray(images), np.asarray(labels))
```

Загрузим изображения в датасеты

```python
maxsize_w = 100
maxsize_h = 100

maxsize = maxsize_w, maxsize_h

(train_images, train_labels) = load_image_dataset('путь к директории train_set', maxsize)

(test_images, test_labels) = load_image_dataset('путь к директории test_set', maxsize)
```

Посмотрим на параметры обучающей выборки изображений:

```python
train_images.shape
```

Набор меток в обучающей выборке:

```python
print(test_labels)
[0 0 0 1 1 1 0 1 0 1 1 0 0 1]
```

Нам монадобится функция вывода таблицы изображений:

```python
class_names = ['chihuahua', 'muffin']
def display_images(images, labels):
        plt.figure(figsize=(10,10))
        grid_size = min(25, len(images))
        for i in range(grid_size):
                plt.subplot(5, 5, i+1)
                plt.xticks([])
                plt.yticks([])
                plt.grid(False)
                plt.imshow(images[i], cmap=plt.cm.binary)
                plt.xlabel(class_names[labels[i]])
```

Выведем обучающую выборку:

```python
display_images(train_images, train_labels)
plt.show()
```


Для нашего теста будем использовать последовательную модель (т.е. сверточную НС) из библиотеки keras. С помощью класса Sequential можно описать последовательность слоев и их свойства, например следующим образом:
 
```python
from keras.models import Sequential
from keras.layers import Dense, Activation #используем такие типы слоев из библиотеки keras

model = keras.Sequential([
    Dense(32, input_shape=(784,)),
    Activation('relu'),
    Dense(10),
    Activation('softmax'),
])
```

Для последовательного добавлени слоев можно также использовать функцию *add*:

```python
model = Sequential()
model.add(Dense(32, input_dim=784))
model.add(Activation('relu'))
```

Создадим и обучим модель `first_model`:

```python
train_images = train_images / 255.0
test_images = test_images / 255.0

 # Setting up the layers.

first_model = keras.Sequential([
    keras.layers.Flatten(input_shape=(100, 100)),
        keras.layers.Dense(128, activation=tf.nn.sigmoid),
        keras.layers.Dense(16, activation=tf.nn.sigmoid),
    keras.layers.Dense(2, activation=tf.nn.softmax)
])
sgd = keras.optimizers.SGD(lr=0.01, decay=1e-5, momentum=0.7, nesterov=True)

first_model.compile(optimizer=sgd,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

first_model_history=first_model.fit(train_images, train_labels, epochs=100)
```

Оценим точность модели:

```python
test_loss, test_acc = model.evaluate(test_images, test_labels)
print('Test accuracy:', test_acc)
```

Сформируем метки для изображений тестовой выборки по полученной модели и выведем то, что получилось:

```python
predictions = model.predict(test_images)
display_images(test_images, np.argmax(predictions, axis = 1))
plt.show()
```

Создайте альтернативную модель second_model и используйте функцию `fit` для ее обучения:

```python
second_model_history=second_model.fit(train_images, train_labels, epochs=100)
```

Сравните модели 

```python
def plot_history(histories, key='accuracy'):
  plt.figure(figsize=(16,10))

  for name, history in histories:
    val = plt.plot(history.epoch, history.history[key],
                   '--', label=name.title()+' Val')
    plt.plot(history.epoch, history.history[key], color=val[0].get_color(),
             label=name.title()+' Train')

  plt.xlabel('Epochs')
  plt.ylabel(key.replace('_',' ').title())
  plt.legend()

  plt.xlim([0,max(history.epoch)])
```

```python
plot_history([('Первая модель', first_model_history),
              ('Вторая модель', second_model_history)])
```

![](assets/cv_03.png)
**Сравнение истории обучения моделей**


## Задание <a name="5_5"></a>

1) Разработайте 6 различных моделей, меняя количество и типы слоев сверточной НС, количество эпох обучения и свойства изображения.

2) Добейтесь точности распознавания >=92%.



**Дополнительные источники литературы по данному разделу:**


- [Basic classification: Classify images of clothing](https://www.tensorflow.org/tutorials/keras/classification)

- [Что такое Keras](https://neurohive.io/ru/tutorial/nejronnaya-set-keras-python/)

- [List of datasets for machine-learning research](https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research)

- [Как работает сверточная нейронная сеть](https://neurohive.io/ru/osnovy-data-science/glubokaya-svertochnaja-nejronnaja-set/)

- [Курс по машинному обучению](https://github.com/bogdansalyp/ml_course)	

